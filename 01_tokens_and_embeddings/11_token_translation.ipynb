{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40102005",
   "metadata": {},
   "source": [
    "# Translating between tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdb9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT4\n",
    "import tiktoken\n",
    "gpt4tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# BERT\n",
    "from transformers import BertTokenizer\n",
    "berttokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955b313",
   "metadata": {},
   "source": [
    "# attempting a direct translation\n",
    "# GPT4 --> BERT --> GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d1fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue is that they have different tokenizers, \n",
    "# so needs to be translated into text and re-tokenized\n",
    "starting_text = \"Hello, my name is Shre and I like yellow.\"\n",
    "\n",
    "# GPT4's tokens:\n",
    "gpt4_tokens = gpt4tokenizer.encode(starting_text)\n",
    "\n",
    "# bert's tokens\n",
    "bert_tokens = berttokenizer.encode(starting_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6239ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text:\n",
      "Hello, my name is Shre and I like yellow.\n",
      "\n",
      "\n",
      "GPT4 tokens:\n",
      "[9906, 11, 856, 836, 374, 1443, 265, 323, 358, 1093, 14071, 13]\n",
      "\n",
      "Decoded using GPT4:\n",
      "Hello, my name is Shre and I like yellow.\n",
      "\n",
      "Decoded using BERT:\n",
      "lately [unused10] [unused851] [unused831] [unused369] ვ [unused260] [unused318] [unused353] ¾ wan [unused12]\n",
      "\n",
      "\n",
      "BERT tokens:\n",
      "[101, 7592, 1010, 2026, 2171, 2003, 14021, 2890, 1998, 1045, 2066, 3756, 1012, 102]\n",
      "\n",
      "Decoded using BERT:\n",
      "[CLS] hello, my name is shre and i like yellow. [SEP]\n",
      "\n",
      "Decoded using GPT4:\n",
      "�.deleteceptionrgretoin\\M health(idate\tfor deviceinclude�\n"
     ]
    }
   ],
   "source": [
    "print(f'Starting text:\\n{starting_text}')\n",
    "print(f'\\n\\nGPT4 tokens:\\n{gpt4_tokens}')\n",
    "print(f\"\\nDecoded using GPT4:\\n{gpt4tokenizer.decode(gpt4_tokens)}\")\n",
    "print(f\"\\nDecoded using BERT:\\n{berttokenizer.decode(gpt4_tokens)}\")\n",
    "\n",
    "print(f'\\n\\nBERT tokens:\\n{bert_tokens}')\n",
    "print(f\"\\nDecoded using BERT:\\n{berttokenizer.decode(bert_tokens)}\")\n",
    "print(f\"\\nDecoded using GPT4:\\n{gpt4tokenizer.decode(bert_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d8891",
   "metadata": {},
   "source": [
    "# the right way to translate (numbers to text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a737cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello, my name is shre and i like yellow. [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text -> GPT4 tokens -> text -> BERT tokens\n",
    "\n",
    "# 1) to GPT4 tokens\n",
    "starting_text = \"Hello, my name is Shre and I like yellow.\"\n",
    "gpt4_tokens = gpt4tokenizer.encode(starting_text)\n",
    "\n",
    "# 2) back to text\n",
    "gpt4_recon_text = gpt4tokenizer.decode(gpt4_tokens)\n",
    "\n",
    "# 3) then to bert tokens\n",
    "bert_tokens = berttokenizer.encode(gpt4_recon_text)\n",
    "\n",
    "# 4) show the reconstruction\n",
    "berttokenizer.decode(bert_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baacb80",
   "metadata": {},
   "source": [
    "# possible annoyances and confusions in translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d37a3941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text contains 445 characters,\n",
      "              96 GPT4 tokens, and\n",
      "              160 Bert tokens.\n"
     ]
    }
   ],
   "source": [
    "# warning about sizes:\n",
    "txt = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'\n",
    "print(f'Text contains {len(txt)} characters,')\n",
    "print(f'              {len(gpt4tokenizer.encode(txt))} GPT4 tokens, and')\n",
    "print(f'              {len(berttokenizer.encode(txt))} Bert tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b457c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction in BERT:\n",
      "  [101, 2707, 2203, 102]\n",
      "  [CLS] start end [SEP]\n",
      "\n",
      "Reconstruction in GPT4:\n",
      "  [2527, 881, 81923, 881, 4660, 319, 201, 408]\n",
      "  start\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\n",
      "\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# another source of confusion\n",
    "txt = 'start\\r\\n\\r\\n\\r\\n\\n\\r\\n\\r\\n\\t\\t\\t\\n\\r\\n\\rend'\n",
    "# txt = 'start\\t\\t\\t\\t\\t\\t\\tend'\n",
    "# txt = 'start                    end'\n",
    "\n",
    "bert_tokens = berttokenizer.encode(txt)\n",
    "gpt4_tokens = gpt4tokenizer.encode(txt)\n",
    "\n",
    "print(f'Reconstruction in BERT:\\n  {bert_tokens}\\n  {berttokenizer.decode(bert_tokens)}\\n')\n",
    "print(f'Reconstruction in GPT4:\\n  {gpt4_tokens}\\n  {gpt4tokenizer.decode(gpt4_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988aa77",
   "metadata": {},
   "source": [
    "# write translation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1163385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation functions\n",
    "\n",
    "def bert2gpt4(bert_tokens):\n",
    "    b = berttokenizer.decode(bert_tokens)\n",
    "    g = gpt4tokenizer.encode(b)\n",
    "    return g\n",
    "\n",
    "\n",
    "def gpt42bert(gpt4_tokens):\n",
    "    g = gpt4tokenizer.decode(gpt4_tokens)\n",
    "    b = berttokenizer.encode(g)\n",
    "    return b[1:-1] # bert auto-adds [CLS] ... [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1133d285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 88816, 60, 602, 6562, 18414, 1051, 19087, 13, 510, 82476, 60]\n",
      "[1045, 4299, 7967, 2020, 4589, 1012]\n"
     ]
    }
   ],
   "source": [
    "# checking that it goves no errors\n",
    "text = \"I wish chocolate were orange.\"\n",
    "\n",
    "print(bert2gpt4(berttokenizer.encode(text)))\n",
    "print(gpt42bert(gpt4tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0df19e",
   "metadata": {},
   "source": [
    "# BERT --> GPT4 --> BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d12f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text\n",
      "  I wanted to paste in a thought-provoking quote here, but I didn't.\n",
      "\n",
      "BERT Tokens:\n",
      " [101, 1045, 2359, 2000, 19351, 1999, 1037, 2245, 1011, 4013, 22776, 14686, 2182, 1010, 2021, 1045, 2134, 1005, 1056, 1012, 102]\n",
      "\n",
      "BERT to GPT4:\n",
      "  [CLS] i wanted to paste in a thought - provoking quote here, but i didn't. [SEP]\n",
      "\n",
      "Back to BERT:\n",
      " [CLS] i wanted to paste in a thought - provoking quote here, but i didn't. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "text = \"I wanted to paste in a thought-provoking quote here, but I didn't.\"\n",
    "print(f'Original text\\n  {text}\\n')\n",
    "\n",
    "# initial encoding\n",
    "bert_toks = berttokenizer.encode(text)\n",
    "print(f\"BERT Tokens:\\n {bert_toks}\\n\")\n",
    "\n",
    "# translate to GPT4\n",
    "b2g = bert2gpt4(bert_toks)\n",
    "print(f\"BERT to GPT4:\\n  {gpt4tokenizer.decode(b2g)}\\n\")\n",
    "\n",
    "# back-translate to BERT\n",
    "back2bert = gpt42bert(b2g)\n",
    "print(f\"Back to BERT:\\n {berttokenizer.decode(back2bert)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cb0c1",
   "metadata": {},
   "source": [
    "# GPT4 --> BERT --> GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075a57f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text\n",
      "  I still don't have a good quote here. Now it's too late.\n",
      "\n",
      "GPT4 tokens:\n",
      "  [40, 2103, 1541, 956, 617, 264, 1695, 12929, 1618, 13, 4800, 433, 596, 2288, 3389, 13]\n",
      "\n",
      "GPT4 to BERT:\n",
      "  i still don't have a good quote here. now it's too late.\n",
      "\n",
      "Back to GPT4:\n",
      "  i still don't have a good quote here. now it's too late.\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "text = \"I still don't have a good quote here. Now it's too late.\"\n",
    "print(f'Original text\\n  {text}\\n')\n",
    "\n",
    "# initial encoding\n",
    "gpt4Tox = gpt4tokenizer.encode(text)\n",
    "print(f'GPT4 tokens:\\n  {gpt4Tox}\\n')\n",
    "\n",
    "# translate to BERT\n",
    "g2b = gpt42bert(gpt4Tox)\n",
    "print(f'GPT4 to BERT:\\n  {berttokenizer.decode(g2b)}\\n')\n",
    "\n",
    "# back-translate to GPT4\n",
    "back2gpt4 = bert2gpt4(g2b)\n",
    "print(f'Back to GPT4:\\n  {gpt4tokenizer.decode(back2gpt4)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-deep-dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
