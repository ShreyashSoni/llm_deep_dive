{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96af9c57",
   "metadata": {},
   "source": [
    "# Tokenization compression ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba7276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import string\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afb5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4's Tokenizer\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc7f37",
   "metadata": {},
   "source": [
    "# get the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac86319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all books have the same url format;\n",
    "# they are unique by numerical code\n",
    "baseurl = 'https://www.gutenberg.org/cache/epub/'\n",
    "\n",
    "bookurls = [\n",
    "    # code       title\n",
    "    ['84',    'Frankenstein'    ],\n",
    "    ['64317', 'GreatGatsby'     ],\n",
    "    ['11',    'AliceWonderland' ],\n",
    "    ['1513',  'RomeoJuliet'     ],\n",
    "    ['76',    'HuckFinn'        ],\n",
    "    ['219',   'HeartDarkness'   ],\n",
    "    ['2591',  'GrimmsTales'     ],\n",
    "    ['2148',  'EdgarAllenPoe'   ],\n",
    "    ['36',    'WarOfTheWorlds'  ],\n",
    "    ['829',   'GulliversTravels']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627de30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Book title     |  Chars  |  Tokens | Compression\n",
      "--------------------------------------------------\n",
      "Frankenstein     | 446,544 | 102,419 | 22.94%\n",
      "GreatGatsby      | 296,858 |  70,343 | 23.70%\n",
      "AliceWonderland  | 167,674 |  41,457 | 24.72%\n",
      "RomeoJuliet      | 167,429 |  43,761 | 26.14%\n",
      "HuckFinn         | 602,714 | 159,125 | 26.40%\n",
      "HeartDarkness    | 232,885 |  56,483 | 24.25%\n",
      "GrimmsTales      | 549,736 | 137,252 | 24.97%\n",
      "EdgarAllenPoe    | 632,131 | 144,315 | 22.83%\n",
      "WarOfTheWorlds   | 363,420 |  84,580 | 23.27%\n",
      "GulliversTravels | 611,742 | 143,560 | 23.47%\n"
     ]
    }
   ],
   "source": [
    "print('  Book title     |  Chars  |  Tokens | Compression')\n",
    "print('-'*50)\n",
    "\n",
    "for code, title in bookurls:\n",
    "    # get the text\n",
    "    fullurl = baseurl + code + '/pg' + code + '.txt'\n",
    "    text = requests.get(fullurl).text\n",
    "    num_chars = len(text)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenizer.encode(text)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    # compression ratio\n",
    "    compress = (num_tokens / num_chars) * 100\n",
    "\n",
    "    print(f\"{title:16} | {num_chars:>7,d} | {num_tokens:>7,d} | {compress:>3.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658dd378",
   "metadata": {},
   "source": [
    "# let's see some websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e40c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weburls = [\n",
    "    'http://python.org/',\n",
    "    'https://pytorch.org/',\n",
    "    'https://en.wikipedia.org/wiki/List_of_English_words_containing_Q_not_followed_by_U',\n",
    "    'https://sudoku.com/',\n",
    "    'https://reddit.com/',\n",
    "    'https://visiteurope.com/en/',\n",
    "    'https://sincxpress.com/',\n",
    "    'https://openai.com/',\n",
    "    'https://theuselessweb.com/',\n",
    "    'https://maps.google.com/',\n",
    "    'https://pigeonsarentreal.co.uk/',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e97165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Website        |  Chars  |  Tokens | Compression\n",
      "-----------------------------------------------------\n",
      "python             |  50,332 |  12,791 |  25.41%\n",
      "pytorch            | 388,061 | 111,398 |  28.71%\n",
      "en.wikipedia       |      92 |      26 |  28.26%\n",
      "sudoku             | 139,673 |  51,094 |  36.58%\n",
      "reddit             | 479,437 | 148,207 |  30.91%\n",
      "visiteurope        | 116,010 |  31,668 |  27.30%\n",
      "sincxpress         |  25,580 |   6,843 |  26.75%\n",
      "openai             |  11,617 |   6,461 |  55.62%\n",
      "theuselessweb      |   4,756 |   1,329 |  27.94%\n",
      "maps.google        | 212,313 | 107,456 |  50.61%\n",
      "pigeonsarentreal.c | 243,854 |  71,232 |  29.21%\n"
     ]
    }
   ],
   "source": [
    "print('    Website        |  Chars  |  Tokens | Compression')\n",
    "print('-'*53)\n",
    "\n",
    "for url in weburls:\n",
    "\n",
    "    # get the text\n",
    "    text = requests.get(url).text\n",
    "    num_chars = len(text)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = tokenizer.encode(text)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    # compression ratio\n",
    "    compress = 100*num_tokens/num_chars\n",
    "\n",
    "    print(f'{urlparse(url).hostname[:-4]:18} | {num_chars:>7,d} | {num_tokens:>7,d} |  {compress:>3.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451f45f",
   "metadata": {},
   "source": [
    "# using the 'string' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e16ce9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attribute     |  Chars  |  Tokens | Compression\n",
      "--------------------------------------------------\n",
      "__name__        |       6 |       1 |  16.67%\n",
      "__doc__         |     622 |     109 |  17.52%\n",
      "__package__     |       6 |       1 |  16.67%\n",
      "__file__        |      84 |      22 |  26.19%\n",
      "__cached__      |     109 |      31 |  28.44%\n",
      "whitespace      |       6 |       4 |  66.67%\n",
      "ascii_lowercase |      26 |       1 |   3.85%\n",
      "ascii_uppercase |      26 |       1 |   3.85%\n",
      "ascii_letters   |      52 |       2 |   3.85%\n",
      "digits          |      10 |       4 |  40.00%\n",
      "hexdigits       |      22 |       7 |  31.82%\n",
      "octdigits       |       8 |       3 |  37.50%\n",
      "punctuation     |      32 |      21 |  65.62%\n",
      "printable       |     100 |      31 |  31.00%\n"
     ]
    }
   ],
   "source": [
    "print('  Attribute     |  Chars  |  Tokens | Compression')\n",
    "print('-'*50)\n",
    "\n",
    "for k,v in string.__dict__.items():\n",
    "    if isinstance(v,str) and (len(v)>0):\n",
    "\n",
    "        # get the text\n",
    "        num_chars = len(v)\n",
    "\n",
    "        # tokenize\n",
    "        tokens = tokenizer.encode(v)\n",
    "        num_tokens = len(tokens)\n",
    "\n",
    "        # compression ratio\n",
    "        compress = 100*num_tokens/num_chars\n",
    "\n",
    "        print(f'{k:15} | {num_chars:>7,d} | {num_tokens:>7,d} |  {compress:>5.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee6d9210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A collection of string constants.\\n\\nPublic module variables:\\n\\nwhitespace -- a string containing all ASCII whitespace\\nascii_lowercase -- a string containing all ASCII lowercase letters\\nascii_uppercase -- a string containing all ASCII uppercase letters\\nascii_letters -- a string containing all ASCII letters\\ndigits -- a string containing all ASCII decimal digits\\nhexdigits -- a string containing all ASCII hexadecimal digits\\noctdigits -- a string containing all ASCII octal digits\\npunctuation -- a string containing all ASCII punctuation characters\\nprintable -- a string containing all ASCII characters considered printable\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.,\n",
    "string.__dict__['__doc__']\n",
    "# string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d4ce3",
   "metadata": {},
   "source": [
    "# Tokenization in different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833db0f4",
   "metadata": {},
   "source": [
    "Tokenization ≠ compression (but it often is).\n",
    "\n",
    "Languages that have more complex written forms (e.g., morphemes in Chinese, richer morphology in Tamil) may require more tokens that characters.\n",
    "\n",
    "Tokenization is less effective in languages they have less training data on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11fe9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizerB = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae04335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['English','Hindi','Spanish','Arabic','Persian','Lithuanian','Chinese','Tamil','Esperanto']\n",
    "\n",
    "sentences = [ 'Blue towels are great because they remind you of the sea, although the sea is wet and towels work better when they are dry.',\n",
    "              'नीले तौलिए बहुत अच्छे होते हैं क्योंकि वे आपको समुद्र की याद दिलाते हैं, हालांकि समुद्र गीला होता है और तौलिए सूखे होने पर बेहतर काम करते हैं।',\n",
    "              'Las toallas azules son geniales porque recuerdan al mar, aunque el mar está mojado y las toallas funcionan mejor cuando están secas.',\n",
    "              'تعتبر المناشف الزرقاء رائعة لأنها تذكرك بالبحر، على الرغم من أن البحر مبلل والمناشف تعمل بشكل أفضل عندما تكون جافة.',\n",
    "              'حوله‌های آبی عالی هستند زیرا شما را به یاد دریا می‌اندازند، اگرچه دریا مرطوب است و حوله‌ها وقتی خشک باشند بهتر عمل می‌کنند.',\n",
    "              'Mėlyni rankšluosčiai puikūs, nes primena jūrą, nors jūra yra šlapia, o rankšluosčiai geriau tinka, kai yra sausi.',\n",
    "              '蓝色毛巾很棒，因为它们会让您想起大海，尽管海水是湿的，而毛巾在干燥时效果更好。',\n",
    "              'நீல நிற துண்டுகள் சிறந்தவை, ஏனென்றால் அவை கடலை நினைவூட்டுகின்றன, இருப்பினும் கடல் ஈரமாக இருக்கும், துண்டுகள் உலர்ந்திருக்கும் போது சிறப்பாக வேலை செய்யும்.',\n",
    "              'Bluaj mantukoj estas bonegaj ĉar ili memorigas vin pri la maro, kvankam la maro estas malseka kaj mantukoj funkcias pli bone kiam ili estas sekaj.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45006611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Language  |  Chars  |  BERT  |  GPT \n",
      "-------------------------------------\n",
      "   English |   123   |   26   |   26\n",
      "     Hindi |   142   |   78   |  136\n",
      "   Spanish |   132   |   46   |   34\n",
      "    Arabic |   115   |   95   |   84\n",
      "   Persian |   123   |   96   |   90\n",
      "Lithuanian |   113   |   46   |   57\n",
      "   Chinese |    39   |   39   |   55\n",
      "     Tamil |   154   |   35   |  209\n",
      " Esperanto |   146   |   56   |   50\n"
     ]
    }
   ],
   "source": [
    "# table header\n",
    "print(' Language  |  Chars  |  BERT  |  GPT ')\n",
    "print('-'*37)\n",
    "\n",
    "for lang, text in zip(languages, sentences):\n",
    "    # tokenize the text\n",
    "    tokens_g = tokenizer.encode(text)\n",
    "    tokens_b = tokenizerB.encode(text)[1:-1]\n",
    "\n",
    "    print(f\"{lang:>10} |   {len(text):3}   |  {len(tokens_b):3}   |  {len(tokens_g):3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe4d25d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-deep-dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
