{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699735ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1034d",
   "metadata": {},
   "source": [
    "# initialize the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd78d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' ' appears 7 times.\n",
      "'a' appears 1 times.\n",
      "'e' appears 5 times.\n",
      "'g' appears 5 times.\n",
      "'h' appears 4 times.\n",
      "'i' appears 3 times.\n",
      "'k' appears 2 times.\n",
      "'l' appears 5 times.\n",
      "'n' appears 1 times.\n",
      "'o' appears 2 times.\n",
      "'r' appears 2 times.\n",
      "'s' appears 2 times.\n",
      "'t' appears 1 times.\n",
      "'u' appears 3 times.\n",
      "'v' appears 2 times.\n",
      "'y' appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "# some text with lots of repetitions\n",
    "text = 'like liker love lovely hug hugs hugging hearts'\n",
    "\n",
    "chars = list(set(text))\n",
    "chars.sort() # initial vocab is sorted\n",
    "\n",
    "for l in chars:\n",
    "    print(f\"'{l}' appears {text.count(l)} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff9c3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'e': 2,\n",
       " 'g': 3,\n",
       " 'h': 4,\n",
       " 'i': 5,\n",
       " 'k': 6,\n",
       " 'l': 7,\n",
       " 'n': 8,\n",
       " 'o': 9,\n",
       " 'r': 10,\n",
       " 's': 11,\n",
       " 't': 12,\n",
       " 'u': 13,\n",
       " 'v': 14,\n",
       " 'y': 15}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a vocab\n",
    "vocab = {word: i for i, word in enumerate(chars)}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4b53c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like liker love lovely hug hugs hugging hearts\n",
      "['l', 'i', 'k', 'e', ' ', 'l', 'i', 'k', 'e', 'r', ' ', 'l', 'o', 'v', 'e', ' ', 'l', 'o', 'v', 'e', 'l', 'y', ' ', 'h', 'u', 'g', ' ', 'h', 'u', 'g', 's', ' ', 'h', 'u', 'g', 'g', 'i', 'n', 'g', ' ', 'h', 'e', 'a', 'r', 't', 's']\n"
     ]
    }
   ],
   "source": [
    "# the text needs to be a list, not a string\n",
    "# each element in the list is a token\n",
    "original_text = list(text)\n",
    "print(text)\n",
    "print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657da2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_stats(text2pair):\n",
    "    token_pairs = dict()\n",
    "\n",
    "    for i in range(len(text2pair)-1):\n",
    "        pair = text2pair[i] + text2pair[i+1]\n",
    "\n",
    "        if pair in token_pairs:\n",
    "            token_pairs[pair] += 1\n",
    "        else:\n",
    "            token_pairs[pair] = 1\n",
    "    \n",
    "    return token_pairs\n",
    "\n",
    "\n",
    "def update_vocab(token_pairs, vocab):\n",
    "    idx = np.argmax(list(token_pairs.values()))\n",
    "    new_token = list(token_pairs.keys())[idx]\n",
    "\n",
    "    vocab[new_token] = max(vocab.values()) + 1\n",
    "\n",
    "    return vocab, new_token\n",
    "\n",
    "\n",
    "def generate_new_token_sequence(prev_text, new_token):\n",
    "    new_text = []\n",
    "\n",
    "    i = 0\n",
    "    while i < (len(prev_text)-1):\n",
    "        if (prev_text[i] + prev_text[i+1]) == new_token:\n",
    "            new_text.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_text.append(prev_text[i])\n",
    "            i += 1\n",
    "        \n",
    "    if i < len(prev_text):\n",
    "        new_text.append(prev_text[i])\n",
    "    \n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de4d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize the vocab (again...)\n",
    "vocab = { word:i for i,word in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86283402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab has 17 tokens after adding ' h'\n",
      "Vocab has 18 tokens after adding ' l'\n",
      "Vocab has 19 tokens after adding ' hu'\n",
      "Vocab has 20 tokens after adding ' hug'\n",
      "Vocab has 21 tokens after adding 'ik'\n",
      "Vocab has 22 tokens after adding 'ike'\n",
      "Vocab has 23 tokens after adding ' lo'\n",
      "Vocab has 24 tokens after adding ' lov'\n",
      "Vocab has 25 tokens after adding ' love'\n"
     ]
    }
   ],
   "source": [
    "# how many tokens in the vocab?\n",
    "vocab_size = 25\n",
    "\n",
    "# make a copy of original text\n",
    "updated_text = original_text.copy()\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    # get pair statistics\n",
    "    pairs = get_pair_stats(updated_text)\n",
    "    # update the dictionary\n",
    "    vocab, new_token = update_vocab(pairs, vocab)\n",
    "    # get a new list of tokens\n",
    "    updated_text = generate_new_token_sequence(updated_text, new_token)\n",
    "    print(f\"Vocab has {len(vocab)} tokens after adding '{new_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3be1832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'ike', ' l', 'ike', 'r', ' love', ' love', 'l', 'y', ' hug', ' hug', 's', ' hug', 'g', 'i', 'n', 'g', ' h', 'e', 'a', 'r', 't', 's']\n"
     ]
    }
   ],
   "source": [
    "# final tokenized text\n",
    "print(updated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c41be5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'a': 1,\n",
       " 'e': 2,\n",
       " 'g': 3,\n",
       " 'h': 4,\n",
       " 'i': 5,\n",
       " 'k': 6,\n",
       " 'l': 7,\n",
       " 'n': 8,\n",
       " 'o': 9,\n",
       " 'r': 10,\n",
       " 's': 11,\n",
       " 't': 12,\n",
       " 'u': 13,\n",
       " 'v': 14,\n",
       " 'y': 15,\n",
       " ' h': 16,\n",
       " ' l': 17,\n",
       " ' hu': 18,\n",
       " ' hug': 19,\n",
       " 'ik': 20,\n",
       " 'ike': 21,\n",
       " ' lo': 22,\n",
       " ' lov': 23,\n",
       " ' love': 24}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the final vocab\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23137e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-deep-dive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
